# Results

```{r, include=FALSE}
library(GGally)
library(tidyverse)
library(readxl)
library(skimr)
library(ggplot2)
library(ggridges)
library(ggpubr)
library(formattable)
library(RColorBrewer)
library(moments)
library(echarts4r)
library(abdiv)
library(lmtest)
library(bestglm)
```

```{r, include=FALSE}
school <- read_csv("../School.csv")
district <- read_csv("../District.csv")
```

## Analyzing standardized test scores

In order to investigate the larger question of how student demographics impact test scores, we need to take analyze the test scores themselves.

### Descriptive Statistics

First, we'll start off by looking at a few descriptive statistics to see what they tell us about the Math and ELA test scores. The tables below contain basic descriptive statistics for each exam over the years. Each data point is the mean test score at a given school from grades 3-8 for the respective exam.

```{r}

get_desc_stat = function(df, year, stat, subject) {
  
  ## Filtering by subject & year: 
  
  if (subject == "Math") {
    data = df |> filter(Year == year) |> pull(`Mean Scale Score_m`)
  }
  else if (subject == "ELA") {
    data =  df |> filter(Year == year) |> pull(`Mean Scale Score_e`)
  }
  
  ## Retrieving the desired stat for the desired subject:

 if (stat == "var") {
   desc_stat = var(data) |> round(digits = 2)
 }
  
  else if (stat == "median") {
    desc_stat = median(data)  |> round (digits = 2)
  }
  
  else if (stat == "mean") {
    desc_stat = mean(data)  |> round (digits = 2)
  } 
  
  else if (stat == "max") {
    desc_stat = max(data)  |> round (digits = 2)
  }
  
  else if (stat == "min") {
    desc_stat = min(data)  |> round (digits = 2)
  } 
  
  else if (stat == "kurtosis") {
    desc_stat = kurtosis(data)  |> round (digits = 2)
  }
  
  else if (stat == "skewness") {
    desc_stat = skewness(data)  |> round (digits = 2)
  }
  
  else if (stat == "Shapiro-Wilk p value") {
    desc_stat = shapiro.test(data)$p.value  |> round (digits = 2)
  }
  
  
  else {
    stop("Invalid stat provided")
  }
  return(desc_stat)
}


get_stat_matrix = function(df, years, stats, subject) {
  
  stat_matrix = matrix(data = NA, nrow = length(stats), ncol = length(years))
  
  for (j in 1:length(years)) {
    
    for (i in 1:length(stats)) {
      
      stat_matrix[i,j] = get_desc_stat(df, years[j], stats[i], subject)
      
    }
    
  }
  
  stat_df = data.frame(stat_matrix)
  colnames(stat_df) = years
  rownames(stat_df) = stats
  return(stat_df)
}

desc_stats = c("min", "max", "median", "var", "skewness", "Shapiro-Wilk p value")
test_years = c("2018", "2019", "2022")



knitr::kable(formattable(get_stat_matrix(school, test_years, desc_stats, "Math")), caption = "Descriptive Statistics for Math Scores by Year")

```

Here are a few notes about this table:

-   The p-value for our Shapiro-Wilk normality test is approximately 0 each year. This means that we have significant evidence to reject the null-hypothesis that our data is normally distributed. Since our scores are not normal in any year, we must keep this in mind for any analyses/visualizations done with this data.

-   2022 was an interesting year because it had the lowest max and median, yet it still had the highest variance. This might suggest that scores were lower than previous years, yet more spread out. The skewness was also lowest closest to 0 in 2022, which suggests our scores were more symmetrical than in previous years. 

-   Skewness values above 0.5 in 2018 and 2019 means the data was slightly positively skewed in those years. In other words, the scores were slightly more concentrated at the higher end of the scale in these years. 





```{r}
knitr::kable(formattable(get_stat_matrix(school, test_years, desc_stats, "ELA")), caption = "Descriptive Statistics for ELA Scores by Year")
```

Looking at the table for our ELA scores, here are a few more takeaways:

-   Again, p-value for our Shapiro-Wilk normality test is approximately 0 each year. This means our ELA scores are also not normal in any year.
-   The data follows a similar trend to Math scores in that 2022 had the highest variance and lowest skew of the years observed. 
-   The variances among ELA scores was comparatively lower than the corresponding variances for Math scores each year, though they followed similar trends over the years. The same is true for skewness. This means the scores for ELA were relatively less spread out and more symmetrical. 


### Reading vs. Math Scatterplot

In investigating the relationship between Reading and Math scores, we started with the big picture question: <b> is there a correlation between success on Math & success on ELA? </b> We chose to break this down at the school level, because there is more data for us to work with compared to scores at the district level.

   

```{r}
r_v_m <- ggplot(data = school) +
       geom_point(aes(x = school$`Mean Scale Score_e`, y = school$`Mean Scale Score_m`), color = "lightskyblue", alpha =0.5) + 
       xlab('Mean ELA Score') +
       ylab('Mean Math Score') +
       labs(title = 'Reading vs Math Scores by School', subtitle = "Faceted by Year") +
       facet_wrap(~Year) + 
       theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
r_v_m + geom_abline(color = "navyblue")

```

   

The graphs above are three scatterplots of reading vs math test scores, separated based on the year the test was taken. The first main takeaway here is that there seems to be a strong positive correlation between reading and math scores in all three years, as one might expect. In other words, schools that performed well in math usually did well on reading too, and vice-versa. There were a few outliers each year, where some schools did noticeably better on math than on reading. There were also a few, less noticeable, schools who performed slightly better on ELA than on math.

The second big takeaway from these scatterplots is that math scores were noticeably lower in 2022 than in years prior. There is also more variance in scores in 2022, which is demonstrated by higher variance of our scatterplot than in the other two years. We can likely attribute this to the onset of the Covid-19 global pandemic, which abruptly forced students to adapt to online learning for anywhere between six months to an entire school year.

   

### Correlation Analysis

Now that we have examined the linear relationship between Math and ELA scores graphically, let's see if we can find any more information by doing so analytically. 

We will use the Spearman's correlation to examine the strength of the linear relationship between scores in each year since they are not normally distributed. The data meets the assumptions for calculating Spearman's correlation coefficient because test scores each year are on an interval scale, they represent paired observations, and they have a monotonic relationship (as shown above). This is what we get:

   

```{r, echo = FALSE, warning=FALSE}
get_spearman_corr = function(df, year) {
  math = df |> filter(Year == year) |> pull(`Mean Scale Score_m`)
  ela = df |> filter(Year == year) |> pull(`Mean Scale Score_e`)
  spearman_result <- cor.test(math, ela, method = "spearman")$estimate |> round(3)
  return(spearman_result)
}

get_corr_matrix = function(df, years) {
  corr_matrix = matrix(data = NA, nrow = 1, ncol = length(years))
  for (i in 1:length(years)) {
      corr = get_spearman_corr(df, years[i])
      corr_matrix[1,i] = corr
  }
  corr_df = data.frame(corr_matrix)
  colnames(corr_df) = years
  rownames(corr_df) = "Spearman correlation coefficient"
  return(corr_df)
}

formattable(get_corr_matrix(school, c("2018", "2019", "2022")))

```

   

Looking at this table, we can see that the linear relationship between scores on the two exams is fairly strong, as expected. Furthermore, the correlation coefficient was slightly lower in 2022 as we saw from our scatterplot. Generally speaking, we know that a relatively high achieving Math school also did well on ELA (and vice-versa). However, this leads us to wonder <b> is there a difference between Math and ELA scores over the years?</b>

   

### Density Histogram of Test Scores

   

Again, we will start with a visualization to investigate the question of which exam students actually performed better on, over the years. Again, we are looking at this data on the school level because there is more data to work with, and thus, more nuance to be uncovered.

   

```{r}
scores = school |> select(Year, `Mean Scale Score_e`, `Mean Scale Score_m`, DBN) |>
  pivot_longer(cols = c("Mean Scale Score_e", "Mean Scale Score_m")) |>
  mutate(Test = case_when(name == "Mean Scale Score_e" ~ "ELA",
                   name == "Mean Scale Score_m" ~ "Math")) |>
           dplyr::select(-"name") |> 
           rename(score = value, year = Year)


ggplot(data = scores, aes(score, fill = Test)) + 
  geom_density(alpha = 0.5) + 
  facet_wrap(~year) + 
  theme_bw(14) + 
  scale_fill_manual(values = c("ELA" = "lightskyblue", "Math" = "navyblue"))
  
```

   

Looking at our density histograms, we can see that there is a lot of overlap between scores in both subjects in this three year sample size. At first glance, the relatively higher concentration of ELA scores from 600 onwards suggests that student's performed slightly better on ELA than Math each year. This is particularly true in 2022 where scores on the exams are most distinguishable. 

   

### Confidence Intervals

   

We can utilize the Wilcox Test to estimate the difference between median Math and ELA scores and determine the significance of said difference. We cannot use the traditional paired t-test because our data is not normal. The following table contains a 99% confidence interval for the difference between median scores in each year, in addition to the p-value for the corresponding test. Our null-hypothesis is that the two sample means are equal (or that the test scores for ELA and Math are the same).

   

```{r echo=FALSE}
get_wilcox_test = function(y, df) {
  x = df |> filter(year == y, Test == "Math") |> pull(score)
  y = df |> filter(year == y, Test == "ELA") |> pull(score)
  test = wilcox.test(x,
       y, 
       conf.int = TRUE, 
       conf_level = 0.99, 
       paired = TRUE,
       mu = 0)
  return(test)
}


get_conf_int_table = function(df) {
  num_years = length(df$year |> unique())
  conf_int_matrix = matrix(data = NA, nrow = num_years, ncol = 3)
  
  for (i in 1:num_years) {
    current_year = unique(df$year)[i]
    conf_int = get_wilcox_test(current_year, df)$conf.int
    conf_int_matrix[i,1] = conf_int[1] |> round(digits = 2)
    conf_int_matrix[i,2] = conf_int[2] |> round(digits = 2)
    conf_int_matrix[i,3] = get_wilcox_test(current_year, df)$p.value |> round(digits = 2)
    
  }
  
  conf_int_df = data.frame(conf_int_matrix)
  rownames(conf_int_df) = df$year |> unique()
  colnames(conf_int_df) = c("Lower boundary", "Upper boundary", "Wilcox test p-value")
  
  return(formattable(conf_int_df))
}

get_conf_int_table(scores)
```

   

Looking at the result of our test for each year reveals: $$p \approx 0 < 0.01$$ Thus, we can reject the null hypothesis that our two sample means are the same, even at the 1% level of significance! Therefore, we have evidence of a statistically significant difference between the median Math & ELA scores in each year observed. 

It is notable that the 99% confidence interval for 2022, (-5.67, -5.16), is much larger than other years. This means we are 99% confident that the true median difference between Math and ELA scores at a given school in 2022 is between -5.67 and -5.16. In the context of standardized testing, this difference in scores is relatively small each year-- however, it is important for our analyses that Math scores are consistently lower than ELA scores, and that this difference is statistically significant each year despite the negligible difference in some years. 

### Main Takeaways

In summary, we know schools that performed relatively well on one exam were also successful on the other. This conclusion is fairly intuitive, though it was still important to analyze the data before assuming so. Furthermore, there is evidence to suggest schools performed better on ELA than Math each year, though this difference was quite small in some years. This is interesting because the linear relationship between scores is very strong, yet there is still a statistically significant difference between scores. Lastly, every test, table, and visualization revealed that 2022 was a fairly odd year. This is likely related to the break from standardized testing due to the onset of the Covid-19 global pandemic. The virus's impact on education will be seen for many years to come and extends far beyond the analysis of standardized test scores done as part of this project. 

   

## Analyzing student demographics

Let's analyze the student demographic data now that we have a clear understanding of standardized test scores. As a reminder, we are only focused on students from grades 3-8. 
   

### Race at the citywide level

   

```{r}

citywide = school |> 
           mutate(Asian = `Total Enrollment`* `% Asian`,
                  Black = `Total Enrollment`* `% Black`,
                  Hispanic = `Total Enrollment`* `% Hispanic`,
                  White = `Total Enrollment`* `% White`,
                  Other = `Total Enrollment`* `% Native American` + `Total Enrollment`* `% Multi-Racial`,
                  ELL = `Total Enrollment`* `% ELL`,
                  SWD = `Total Enrollment`* `% SWD`,
                  Poverty = `Total Enrollment`* `% Poverty`)|> 
           select(Asian, Black, Hispanic, White, Other, Year, ELL, SWD, Poverty, `Total Enrollment`, ENI) |> 
           group_by(Year) |> 
           summarise(Asian = sum(Asian)/sum(`Total Enrollment`), 
                     Black = sum(Black)/sum(`Total Enrollment`),
                     Hispanic = sum(Hispanic)/sum(`Total Enrollment`),
                     White = sum(White)/sum(`Total Enrollment`),
                     Other = sum(Other)/sum(`Total Enrollment`),
                     ELL = sum(ELL)/sum(`Total Enrollment`), 
                     SWD = sum(SWD)/sum(`Total Enrollment`), 
                     Poverty = sum(Poverty)/sum(`Total Enrollment`), 
                     ENI = mean(ENI)) |> 
           mutate(Year = as.factor(Year))
```

```{r}
citywide_agg = school |> 
           mutate(Asian = `Total Enrollment`* `% Asian`,
                  Black = `Total Enrollment`* `% Black`,
                  Hispanic = `Total Enrollment`* `% Hispanic`,
                  White = `Total Enrollment`* `% White`,
                  Other = `Total Enrollment`* `% Native American` + `Total Enrollment`* `% Multi-Racial`,
                  ELL = `Total Enrollment`* `% ELL`,
                  SWD = `Total Enrollment`* `% SWD`,
                  Poverty = `Total Enrollment`* `% Poverty`)|> 
           select(Asian, Black, Hispanic, White, Other, Year, ELL, SWD, Poverty, `Total Enrollment`, ENI) |> 
           summarise(Asian = sum(Asian)/sum(`Total Enrollment`), 
                     Black = sum(Black)/sum(`Total Enrollment`),
                     Hispanic = sum(Hispanic)/sum(`Total Enrollment`),
                     White = sum(White)/sum(`Total Enrollment`),
                     Other = sum(Other)/sum(`Total Enrollment`),
                     ELL = sum(ELL)/sum(`Total Enrollment`), 
                     SWD = sum(SWD)/sum(`Total Enrollment`), 
                     Poverty = sum(Poverty)/sum(`Total Enrollment`), 
                     ENI = mean(ENI))

citywide_donut = citywide_agg |> 
           select(c("Asian", "Black", "Hispanic", "White", "Other")) |> 
           data.table::transpose()

rownames(citywide_donut) = colnames(citywide)[2:6]

citywide_donut = citywide_donut |> 
  rownames_to_column("race") |> 
  rename(percentage = "V1")

citywide_donut |> 
  e_charts(race) |> 
  e_pie(percentage, radius = c("50%", "70%"), itemStyle = list(borderRadius = 10, borderColor = '#fff', borderWidth = 0.5)) |> 
  e_toolbox_feature(feature = "restore") |> 
  e_toolbox_feature(feature = "magicType") |> 
  e_toolbox_feature("dataView") |> 
  e_toolbox_feature("saveAsImage") |> 
  e_tooltip(trigger = "axis") |> 
  e_legend(right = 0, 
           orient = "vertical",
           top = 50) |> 
  e_color(color = brewer.pal(n = 5, name = "Blues")) |> 
  e_title(text = "Citywide Racial Demographics of NYC Students", subtext = "Using aggregated data from 2018, 2019, and 2022", left = "center", top = 5)
  
```

       

### Other demographics at the citywide level

   

```{r}
citywide |> 
    e_charts(Year) |> 
    e_bar(ELL, name = "English language learners", stack = "grp1") |> 
    e_bar(SWD, name = "Students with disabilities", stack = "grp2") |> 
    e_bar(Poverty, name = "Students living in poverty", stack = "grp3") |> 
    e_bar(ENI, name = "Economic Need Index", stack = "grp4") |> 
    e_toolbox_feature(feature = "restore") |> 
    e_toolbox_feature(feature = "magicType", type = list("line")) |> 
    e_toolbox_feature("dataView") |> 
    e_toolbox_feature("saveAsImage") |> 
    e_tooltip(trigger = "axis") |> 
    e_legend(orient = "horizontal",
           bottom = 10) |> 
    e_color(color = brewer.pal(n = 4, name = "Blues"))
```

   

### Descriptive statistics & Diversity scores

   

((Diversity Index) [[https://geographyfieldwork.com/Simpson'sDiversityIndex.htm#](https://geographyfieldwork.com/Simpson'sDiversityIndex.htm#){.uri}:\~:text=Simpson's%20Diversity%20Index%20is%20a,evenness%20increase%2C%20so%20diversity%20increases.&text=The%20value%20of%20D%20ranges%20between%200%20and%201.] )

```{r}

district$Diversity = apply(district[,c("% Asian", "% Black", "% Hispanic", "% White", "% Multi-Racial", "% Native American", "% Missing Race/Ethnicity Data")], 1, simpson)


district_agg = district |> 
               mutate(Asian = `Total Enrollment`* `% Asian`,
                  Black = `Total Enrollment`* `% Black`,
                  Hispanic = `Total Enrollment`* `% Hispanic`,
                  White = `Total Enrollment`* `% White`,
                  Other = `Total Enrollment`* `% Native American` + `Total Enrollment`* `% Multi-Racial`,
                  ELL = `Total Enrollment`* `% ELL`,
                  SWD = `Total Enrollment`* `% SWD`,
                  Poverty = `Total Enrollment`* `% Poverty`)|> 
           select(District, Diversity, Asian, Black, Hispanic, White, ELL, SWD, Poverty, `Total Enrollment`, ENI) |> 
           group_by(District) |> 
           summarise(Asian = sum(Asian)/sum(`Total Enrollment`), 
                     Black = sum(Black)/sum(`Total Enrollment`),
                     Hispanic = sum(Hispanic)/sum(`Total Enrollment`),
                     White = sum(White)/sum(`Total Enrollment`),
                     ELL = sum(ELL)/sum(`Total Enrollment`), 
                     SWD = sum(SWD)/sum(`Total Enrollment`), 
                     Poverty = sum(Poverty)/sum(`Total Enrollment`), 
                     ENI = mean(ENI),
                     Diversity = mean(Diversity)) |> 
            mutate_all(funs(round(., digits = 2)))



get_significant_values = function(df, column) {
  
  if (column != "District") {
  col_mean = df |> pull(column) |> mean()  
  col_sd = df |> pull(column) |> sd()
  lower = col_mean - 1.5 * col_sd
  upper = col_mean + 1.5 * col_sd
  
  val = df |> pull(column)
  lower_vals = val[val <= lower]
  upper_vals = val[val >= upper]
  
  return(list(lower = lower_vals, upper = upper_vals))
    
  }
}




generate_column_formatter = function(df, column) {
  
  formatter("span",
  style = x ~ style(
                    display = "block",
                    padding = "5px",
                    `border-radius` = "2px",
                    `background-color` = ifelse(
                                                x %in% get_significant_values(df, column)$lower, "#015C92",
                                                ifelse(x %in% get_significant_values(df, column)$upper, "#BCE6FF", "")
                                                ),
                     color = ifelse(x %in% unlist(get_significant_values(df, column)), "white", ""),
                    `text-align` = "center",
                    `vertical-align` = "middle"
                    ))
}                           


formatters_list <- lapply(names(district_agg), function(column) {
  generate_column_formatter(district_agg, column)
})

# Name the list elements with the column names
names(formatters_list) <- names(district_agg)

# Apply the formatters to your data frame
formattable(district_agg, formatters_list)

```

       

## Regression Analysis

   

Need to explain which variables I am getting rid of and justify them

```{r}
school$Diversity = apply(school[,c("% Asian", "% Black", "% Hispanic", "% White", "% Multi-Racial", "% Native American", "% Missing Race/Ethnicity Data")], 1, simpson)

lm_school = school |> select(c(`Mean Scale Score_e`, `Mean Scale Score_m`, `% Poverty`, `% ELL`, `% SWD`, `ENI`, `Diversity`, `Total Enrollment`))
```

   

### Linear Regression (English)

   

```{r}
english_reg = lm(`Mean Scale Score_e` ~ `% Poverty` + `% ELL` + `% SWD` + `ENI` + `Diversity` + `Total Enrollment`, data = lm_school)
summary(english_reg)
```

   

```{r}

standardized_eng_residuals <- rstandard(english_reg)

plot(x = english_reg$fitted.values, 
     y = standardized_eng_residuals, 
     main = "Residuals vs Fitted",
     xlab = "Fitted values",
     ylab = "Standardized English Residuals")

abline(h = 0, col = "red", lwd = 2)

bptest(english_reg)


# Q-Q plot
qqnorm(standardized_eng_residuals, 
       main = "Normal Q-Q Plot")
qqline(standardized_eng_residuals, col = "red")

shapiro.test(standardized_eng_residuals)



```

       

### Linear Regression (Math scores)

   

```{r}
math_reg = lm(`Mean Scale Score_m` ~ `% Poverty` + `% ELL` + `% SWD` + `ENI` + `Diversity` + `Total Enrollment`, data = lm_school)
summary(math_reg)
```

   

```{r}
standardized_math_residuals <- rstandard(math_reg)

plot(x = math_reg$fitted.values, 
     y = standardized_math_residuals, 
     main = "Residuals vs Fitted",
     xlab = "Fitted values",
     ylab = "Standardized Math Residuals")

abline(h = 0, col = "red", lwd = 2)

bptest(math_reg)


# Q-Q plot
qqnorm(standardized_math_residuals, 
       main = "Normal Q-Q Plot")
qqline(standardized_math_residuals, col = "red")

shapiro.test(standardized_math_residuals)

```

\*\* Both linear models failed the homoscedasticity (constant variance) and normality assumptions for the standardized residuals. This means we cannot reliably interpret the results from either linear regression model. Next, we will try a Generalized Linear Model (GLM) with a Gamma link function. A Gamma link makes sense here because they are usually used when working with non-negative, continuous, and positive-skewed data.

   

### GLM (English)

       

```{r}
glm_eng = glm(`Mean Scale Score_e` ~ `% Poverty` + `% ELL` + `% SWD` + `ENI` + `Diversity` + `Total Enrollment`,  family = "Gamma"(link = log), data = lm_school)

summary(glm_eng)
```

   

### GLM (Math)

   

```{r}
glm_math = glm(`Mean Scale Score_m` ~ `% Poverty` + `% ELL` + `% SWD` + `ENI` + `Diversity` + `Total Enrollment`,  family = "Gamma"(link = log), data = lm_school)

summary(glm_math)
```

\*\*\* Still need to find out if there are any assumptions i need to make or anything to prove before interpretations \*\*\*

       
